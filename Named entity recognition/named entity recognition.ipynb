{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500c2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF, metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1e0afd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sentence #           Word  POS     Tag\n",
      "0    Sentence: 1      Thousands  NNS       O\n",
      "1            NaN             of   IN       O\n",
      "2            NaN  demonstrators  NNS       O\n",
      "3            NaN           have  VBP       O\n",
      "4            NaN        marched  VBN       O\n",
      "..           ...            ...  ...     ...\n",
      "994          NaN             an   DT       O\n",
      "995          NaN  investigation   NN       O\n",
      "996          NaN             of   IN       O\n",
      "997          NaN         Khayam  NNP  Person\n",
      "998          NaN             's  POS       O\n",
      "\n",
      "[999 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data ='C:/Users/hp/Desktop/Book1.csv'\n",
    "df = pd.read_csv(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24ec4473",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['Word', 'POS']].values.tolist()\n",
    "y = df['Tag'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecfa410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "cv = CountVectorizer(stop_words='english', ngram_range=(1,1), tokenizer=token.tokenize)\n",
    "X = cv.fit_transform(df['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d745104d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x , y,test_size=0.20, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6f4767c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artifact', 'GPE', 'O', 'Organization', 'Person']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(y)\n",
    "classes = classes.tolist()\n",
    "new_classes = classes.copy()\n",
    "new_classes.pop()\n",
    "new_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e1d9ad61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GPE       0.67      0.33      0.44        18\n",
      "           O       0.99      1.00      1.00       172\n",
      "Organization       0.29      0.67      0.40         6\n",
      "      Person       0.33      0.50      0.40         2\n",
      "        Time       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.92       200\n",
      "   macro avg       0.66      0.60      0.58       200\n",
      "weighted avg       0.94      0.92      0.92       200\n",
      "\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GPE', 'GPE', 'O', 'GPE', 'O', 'O', 'O', 'O', 'Organization', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GPE', 'GPE', 'Person', 'O', 'O', 'O', 'O', 'GPE', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GPE', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GPE', 'O', 'O', 'Organization', 'O', 'O', 'O', 'Time', 'Organization', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Person', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'GPE', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=None as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit([X_train], [Y_train])\n",
    "y_pred = crf.predict([X_test]) \n",
    "print(metrics.flat_classification_report([Y_test], y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59e905cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction :  [['O']]\n"
     ]
    }
   ],
   "source": [
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "sentence = [\"Iraq\"]\n",
    "predict = crf.predict([sentence])\n",
    "print('prediction : ' , predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bb7f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def extract_named_entities(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    return entities\n",
    "\n",
    "def predict_entities():\n",
    "    text = entry_input.get()\n",
    "    if text:\n",
    "        entities = extract_named_entities(text)\n",
    "        display_result(entities)\n",
    "    else:\n",
    "        messagebox.showwarning(\"Warning\", \"Please enter some text.\")\n",
    "\n",
    "def display_result(entities):\n",
    "    result_text = \"\"\n",
    "    for entity in entities:\n",
    "        if isinstance(entity, nltk.Tree):\n",
    "            label = entity.label()\n",
    "            value = \" \".join([leaf[0] for leaf in entity.leaves()])\n",
    "            result_text += f\"{label}:\\n- {value}\\n\\n\"\n",
    "    text_output.delete(1.0, tk.END)\n",
    "    text_output.insert(tk.END, result_text)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Named Entity Recognition\")\n",
    "root.geometry(\"600x400\")\n",
    "\n",
    "lbl_input = tk.Label(root, text=\"Enter Text:\")\n",
    "lbl_input.pack()\n",
    "\n",
    "entry_input = tk.Entry(root, width=60)\n",
    "entry_input.pack()\n",
    "\n",
    "btn_predict = tk.Button(root, text=\"Predict Entities\", command=predict_entities)\n",
    "btn_predict.pack(pady=10)\n",
    "\n",
    "text_output = tk.Text(root, wrap=tk.WORD, height=15)\n",
    "text_output.pack(padx=10, pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8274715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "\n",
    "# def print_input():\n",
    "#     input_val = entry.get()\n",
    "#     prediction = crf.predict(input_val)\n",
    "#     print('Prediction : ', prediction)\n",
    "\n",
    "# root = tk.Tk()\n",
    "# root.title(\"Named Entity Recognition\")\n",
    "\n",
    "# label = tk.Label(root, text=\"Enter your text:\")\n",
    "# label.pack()\n",
    "\n",
    "# entry = tk.Entry(root)\n",
    "# entry.pack()\n",
    "\n",
    "# button = tk.Button(root, text=\"Submit\", command=print_input)\n",
    "# button.pack()\n",
    "\n",
    "# root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d0262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
